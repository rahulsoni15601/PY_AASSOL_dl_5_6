{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "                    'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "                    'cifar-10-python.tar.gz',pbar.hook)\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "def _load_label_names():\n",
    "    \"\"\"\n",
    "    Load the label names from file\n",
    "    \"\"\"\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    \"\"\"\n",
    "    Load a batch of the dataset\n",
    "    \"\"\"\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    \"\"\"\n",
    "    Display Stats of the the dataset\n",
    "    \"\"\"\n",
    "    batch_ids = list(range(1, 6))\n",
    "\n",
    "    if batch_id not in batch_ids:\n",
    "        print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
    "        return None\n",
    "\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "\n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch {}:'.format(batch_id))\n",
    "    print('Samples: {}'.format(len(features)))\n",
    "    print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True)))))\n",
    "    print('First 20 Labels: {}'.format(labels[:20]))\n",
    "\n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    label_names = _load_label_names()\n",
    "\n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_image)\n",
    "\n",
    "\n",
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    \"\"\"\n",
    "    Preprocess data and save it to file\n",
    "    \"\"\"\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    \"\"\"\n",
    "    Preprocess Training and Validation Data\n",
    "    \"\"\"\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        validation_count = int(len(features) * 0.1)\n",
    "\n",
    "        # Prprocess and save a batch of training data\n",
    "        _preprocess_and_save(\n",
    "            normalize,\n",
    "            one_hot_encode,\n",
    "            features[:-validation_count],\n",
    "            labels[:-validation_count],\n",
    "            'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # Use a portion of training batch for validation\n",
    "        valid_features.extend(features[-validation_count:])\n",
    "        valid_labels.extend(labels[-validation_count:])\n",
    "\n",
    "    # Preprocess and Save all validation data\n",
    "    _preprocess_and_save(\n",
    "        normalize,\n",
    "        one_hot_encode,\n",
    "        np.array(valid_features),\n",
    "        np.array(valid_labels),\n",
    "        'preprocess_validation.p')\n",
    "\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # load the training data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all training data\n",
    "    _preprocess_and_save(\n",
    "        normalize,\n",
    "        one_hot_encode,\n",
    "        np.array(test_features),\n",
    "        np.array(test_labels),\n",
    "        'preprocess_training.p')\n",
    "\n",
    "\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)\n",
    "\n",
    "\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = _load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "\n",
    "        axies[image_i][0].imshow(feature*255)\n",
    "        axies[image_i][0].set_title(correct_name)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "        axies[image_i][1].set_xticks([0, 0.5, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 9 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 5\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalize(x):\n",
    "    x = np.array(x)\n",
    "    x_max = np.max(x)\n",
    "    x_min = np.min(x)\n",
    "    output = (x-x_min)/(x_max-x_min)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(range(10))\n",
    "def one_hot_encode(x):\n",
    "    return lb.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "\n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "\n",
    "  \n",
    "\n",
    "    # 5, 6\n",
    "\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "\n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "\n",
    "    \n",
    "\n",
    "    # 7, 8\n",
    "\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
    "\n",
    "    \n",
    "\n",
    "    # 9\n",
    "\n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "\n",
    "\n",
    "    # 10\n",
    "\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 11\n",
    "\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "\n",
    "    \n",
    "\n",
    "    # 12\n",
    "\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "\n",
    "    full3 = tf.nn.dropout(full3, keep_prob)\n",
    "\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "\n",
    "    \n",
    "\n",
    "    # 13\n",
    "\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "\n",
    "    full4 = tf.nn.dropout(full4, keep_prob)\n",
    "\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "\n",
    "    \n",
    "\n",
    "    # 14\n",
    "\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "keep_probability = 0.7\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rahul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-27-7cf9140a50fe>:19: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rahul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-27-7cf9140a50fe>:67: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-30-fa63d1594e6a>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = conv_net(x, keep_prob)\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = sess.run(cost,\n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1527 Validation Accuracy: 0.226400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.7834 Validation Accuracy: 0.318600\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.6033 Validation Accuracy: 0.326800\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.5667 Validation Accuracy: 0.402000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.4164 Validation Accuracy: 0.418400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.3811 Validation Accuracy: 0.496800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.1477 Validation Accuracy: 0.461800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     0.9912 Validation Accuracy: 0.524800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.0475 Validation Accuracy: 0.576400\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     0.8776 Validation Accuracy: 0.592800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     0.7739 Validation Accuracy: 0.631200\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.6153 Validation Accuracy: 0.657600\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.3757 Validation Accuracy: 0.660600\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.5993 Validation Accuracy: 0.676200\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.5036 Validation Accuracy: 0.651000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.5161 Validation Accuracy: 0.696000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.3289 Validation Accuracy: 0.686800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.2638 Validation Accuracy: 0.692800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.3423 Validation Accuracy: 0.697800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.2045 Validation Accuracy: 0.692000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.2288 Validation Accuracy: 0.716800\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.2080 Validation Accuracy: 0.692000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.1726 Validation Accuracy: 0.731800\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.2002 Validation Accuracy: 0.713400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.1264 Validation Accuracy: 0.697400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.0992 Validation Accuracy: 0.731600\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.0679 Validation Accuracy: 0.729800\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.0725 Validation Accuracy: 0.730800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.0994 Validation Accuracy: 0.717800\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.0355 Validation Accuracy: 0.698200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.0782 Validation Accuracy: 0.750000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.1366 Validation Accuracy: 0.726800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.0449 Validation Accuracy: 0.725800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.0477 Validation Accuracy: 0.735400\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.0546 Validation Accuracy: 0.690000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.0340 Validation Accuracy: 0.737800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.0139 Validation Accuracy: 0.741600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.0328 Validation Accuracy: 0.709400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.0286 Validation Accuracy: 0.729600\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.0160 Validation Accuracy: 0.731200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.0543 Validation Accuracy: 0.738200\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.0098 Validation Accuracy: 0.737800\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.0037 Validation Accuracy: 0.740800\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.0166 Validation Accuracy: 0.735000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.0126 Validation Accuracy: 0.742200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.0438 Validation Accuracy: 0.722200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.0040 Validation Accuracy: 0.743800\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.0104 Validation Accuracy: 0.727600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.0152 Validation Accuracy: 0.738000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.0101 Validation Accuracy: 0.736200\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5 h4q4g6\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
